name: Full Scraper

on:
  schedule:
    # Run every 12 hours (00:00 UTC and 12:00 UTC)
    - cron: '0 */12 * * *'
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum pages to scrape per site (default: 10, 0 = all pages)'
        required: false
        default: '10'
        type: string

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Allow workflow to commit results back to repo
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium || true

      - name: Run all scrapers
        run: |
          mkdir -p data
          MAX_PAGES="${{ github.event.inputs.max_pages || '10' }}"
          python scripts/run_all_scrapers.py --max-pages $MAX_PAGES --db data/listings.db --excel data/all_listings.xlsx || true

      - name: Commit and push scraped data
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add data/*.xlsx data/*.db 2>/dev/null || true
          if ! git diff-index --quiet HEAD; then
            git commit -m "Auto-scrape: Update listings data $(date +'%Y-%m-%d %H:%M:%S UTC')"
            git push
          else
            echo "No changes to commit"
          fi
